{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brazilian Traffic Law Parser and Structurer\n",
    "\n",
    "## Overview\n",
    "This notebook implements a comprehensive pipeline for parsing and structuring the Brazilian Traffic Law (Código de Trânsito Brasileiro) to create a hierarchical representation suitable for Retrieval-Augmented Generation (RAG) systems.\n",
    "\n",
    "## Key Features\n",
    "- **Law Text Parsing**: Extracts and structures legal text into hierarchical components\n",
    "- **AI-Powered Classification**: Uses OpenAI GPT-4o-mini to identify and classify legal elements\n",
    "- **Hierarchical Organization**: Creates a tree structure following legal document hierarchy\n",
    "- **Vector Database Preparation**: Generates structured data for embedding and retrieval\n",
    "\n",
    "## Legal Hierarchy Structure\n",
    "The pipeline recognizes and organizes the following legal elements in order of precedence:\n",
    "1. **Título** (Title)\n",
    "2. **Capítulo** (Chapter) \n",
    "3. **Seção** (Section)\n",
    "4. **Artigo** (Article)\n",
    "5. **Parágrafo** (Paragraph)\n",
    "6. **Inciso** (Subsection)\n",
    "7. **Item** (Item)\n",
    "\n",
    "## Output\n",
    "- Structured JSON with hierarchical law organization\n",
    "- Vector database dictionary for RAG implementation\n",
    "- Cleaned and filtered legal content ready for AI applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies and Setup\n",
    "\n",
    "### Required Libraries\n",
    "- `openai`: For AI-powered text classification and structuring\n",
    "- `Levenshtein`: For fuzzy string matching and hierarchy mapping\n",
    "- `concurrent.futures`: For parallel processing of law sections\n",
    "- `json`, `os`, `sys`, `re`: Standard Python libraries for file handling and text processing\n",
    "\n",
    "### Prerequisites\n",
    "1. OpenAI API key configured\n",
    "2. Brazilian Traffic Law text file (`cod_transito.txt`) in the working directory\n",
    "3. Output directory structure for processed JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os      # File and directory operations\n",
    "import sys     # System-specific parameters and functions\n",
    "import json    # JSON data handling\n",
    "import re      # Regular expressions for text pattern matching\n",
    "\n",
    "# Third-party imports\n",
    "from Levenshtein import distance              # String similarity calculations\n",
    "from concurrent.futures import ThreadPoolExecutor  # Parallel processing\n",
    "from openai import OpenAI                    # OpenAI API for AI text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "The following libraries are essential for the law parsing pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize content variable\n",
    "content = None\n",
    "\n",
    "# Load the Brazilian Traffic Law text file\n",
    "# Using UTF-8 encoding to handle Portuguese characters properly\n",
    "with open(r\"cod_transito.txt\", 'r', encoding='utf8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Display the loaded content for verification\n",
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Traffic Law Text\n",
    "\n",
    "This section loads the Brazilian Traffic Law text from a local file. The text will be processed and structured in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client with API key\n",
    "# Note: In production, consider using environment variables for API keys\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# System prompt for GPT-4o-mini to structure legal text\n",
    "# This prompt defines the AI's role as a legal text analyzer\n",
    "system_description = \"\"\"Você receberá um trecho de uma lei, a partir dele estruture uma lista com os campos detectados. O tipos dos campos podem ser: Título, Capítulo, Seção, Artigo, Parágrafo, Inciso, item.\n",
    "\n",
    "Regras:\n",
    "- Artigos são escritos na forma \"Art. 1º\" ou \"Art. 1\"\n",
    "- Parágrafos são escritos na forma \"§ 1º\" ou \"§ 1\"\n",
    "- Incisos são escritos na forma \"I\" ou \"1\"\n",
    "- Itens são escritos na forma \"a\" ou \"a)\"\n",
    "- É possível que o trecho não tenha todos os campos, por exemplo, um trecho pode ter apenas um artigo e um parágrafo\n",
    "- Se um texto parecer incompleto finalize o campo com \"...\"\n",
    "\n",
    "O JSON  deve seguir a seguinte estrutura:\n",
    "\n",
    "{\"campos_detectados\":[{\"tipo_do_campo\": str, \"título_do_campo\": str, \"texto\": str},...]}\n",
    "\"\"\"\n",
    "\n",
    "def generate_json_from_law_llm(law_text, output_path):\n",
    "    \"\"\"\n",
    "    Generate structured JSON from legal text using OpenAI GPT-4o-mini.\n",
    "    \n",
    "    This function takes a section of legal text and uses AI to identify and structure\n",
    "    the various legal elements (articles, paragraphs, sections, etc.) into a \n",
    "    standardized JSON format.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    law_text : str\n",
    "        The legal text to be analyzed and structured\n",
    "    output_path : str\n",
    "        Path where the generated JSON file will be saved\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Structured legal elements in JSON format, or None if file already exists\n",
    "    \n",
    "    Notes:\n",
    "    ------\n",
    "    - Uses GPT-4o-mini model for cost-effective processing\n",
    "    - Implements file existence check to avoid reprocessing\n",
    "    - Uses JSON mode to ensure valid JSON output\n",
    "    - Temperature set to 0.3 for consistent, focused responses\n",
    "    \"\"\"\n",
    "    \n",
    "    # Skip processing if output file already exists (avoid unnecessary API calls)\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"Output file {output_path} already exists. Skipping generation.\")\n",
    "        return\n",
    "    \n",
    "    # Make API call to OpenAI GPT-4o-mini\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # Cost-effective model suitable for text analysis\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": system_description\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": law_text\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.3,  # Low temperature for consistent, focused responses\n",
    "        top_p=1,\n",
    "        presence_penalty=0,\n",
    "        response_format={'type': 'json_object'}  # Ensure JSON output format\n",
    "    )\n",
    "    \n",
    "    # Parse the AI response and extract JSON content\n",
    "    text = json.loads(response.to_dict()['choices'][0]['message']['content'])\n",
    "\n",
    "    # Save the structured data to file\n",
    "    with open(output_path, 'w', encoding='utf8') as f:\n",
    "        json.dump(text, f, indent=4)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. OpenAI Setup and Law Structure Detection\n",
    "\n",
    "### AI-Powered Legal Structure Detection\n",
    "This section configures the OpenAI client and defines the core function for detecting legal structure elements using GPT-4o-mini. The AI model is trained to recognize and classify different types of legal elements in Brazilian law.\n",
    "\n",
    "### System Prompt Design\n",
    "The system prompt instructs the AI to:\n",
    "- Identify legal hierarchy elements (Titles, Chapters, Sections, Articles, Paragraphs, etc.)\n",
    "- Extract and structure the content in a consistent JSON format\n",
    "- Handle incomplete or fragmented text gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def particionate_arts(law_text):\n",
    "    \"\"\"\n",
    "    Partition legal text into smaller sections based on articles.\n",
    "    \n",
    "    This function splits the complete law text into manageable chunks, with each chunk\n",
    "    containing one article and its associated content. This segmentation strategy:\n",
    "    1. Reduces token count for AI processing\n",
    "    2. Maintains logical legal structure\n",
    "    3. Enables parallel processing of different articles\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    law_text : str\n",
    "        Complete legal text to be partitioned\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of text segments, each containing one article and related content\n",
    "    \n",
    "    Notes:\n",
    "    ------\n",
    "    - Splits on \"Art. \" pattern which identifies article beginnings\n",
    "    - Preserves the \"Art. \" prefix in each segment (except the first)\n",
    "    - Filters out empty segments\n",
    "    - First segment includes any preamble text before the first article\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    \n",
    "    # Split text by article markers\n",
    "    parts = law_text.split(\"Art. \")\n",
    "    \n",
    "    # Reconstruct segments with proper \"Art. \" prefixes\n",
    "    # First part: preamble + first article (if exists)\n",
    "    # Remaining parts: individual articles with \"Art. \" prefix restored\n",
    "    parts = [parts[0].strip() + \"Art. \" + parts[1].strip()] + [\"Art. \" + part.strip() for part in parts[2:] if part.strip()]\n",
    "    \n",
    "    return parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Partitioning Strategy\n",
    "\n",
    "### Article-Based Segmentation\n",
    "The law text needs to be broken down into manageable chunks for AI processing. This function splits the text by articles (\"Art.\") to create logical segments that can be processed independently while maintaining legal context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the partitioning function to the loaded law content\n",
    "art_list = particionate_arts(content)\n",
    "\n",
    "# Display the list of article segments for verification\n",
    "# This helps verify that the partitioning worked correctly\n",
    "art_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Article Partitioning\n",
    "Execute the partitioning function and examine the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel processing of law articles using ThreadPoolExecutor\n",
    "# This dramatically reduces processing time by handling multiple articles concurrently\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "    # Generate output filenames based on article titles\n",
    "    output_names = []\n",
    "    for art in art_list:\n",
    "        # Extract article title from the first 10 characters\n",
    "        title = art[:10].strip()\n",
    "        \n",
    "        # Clean up title formatting for filename use\n",
    "        if len(title.split()) == 3:\n",
    "            title = title.split()[0] + \" \" + title.split()[1]\n",
    "        \n",
    "        # Remove trailing periods from filenames\n",
    "        if title.endswith('.'):\n",
    "            title = title[:-1]\n",
    "        \n",
    "        # Create JSON filename for each article\n",
    "        output_names.append(f\"{title}.json\")\n",
    "\n",
    "    # Submit all articles for parallel processing\n",
    "    for i, art in enumerate(art_list):\n",
    "        # Define output path for each processed article\n",
    "        output_path = os.path.join(r\"trafficLaw\", output_names[i])\n",
    "        \n",
    "        # Submit article processing task to thread pool\n",
    "        # Each task will call the OpenAI API independently\n",
    "        executor.submit(generate_json_from_law_llm, art, output_path)\n",
    "\n",
    "    # Wait for all tasks to complete before proceeding\n",
    "    executor.shutdown(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parallel AI Processing\n",
    "\n",
    "### Concurrent Processing Strategy\n",
    "This section processes all article segments in parallel using ThreadPoolExecutor to significantly reduce processing time. Each article segment is sent to the OpenAI API concurrently, with results saved as individual JSON files.\n",
    "\n",
    "**Performance Benefits:**\n",
    "- Processes multiple articles simultaneously\n",
    "- Reduces total processing time from hours to minutes\n",
    "- Maintains API rate limits through controlled concurrency (max 20 workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to store all processed JSON data\n",
    "json_files = []\n",
    "\n",
    "# Regular expression to extract numeric values from filenames\n",
    "# Used for proper numerical sorting (e.g., Art 2 before Art 10)\n",
    "get_num = re.compile(r'\\d+')\n",
    "\n",
    "def files_num(file):\n",
    "    \"\"\"Extract numeric value from filename for sorting purposes.\"\"\"\n",
    "    return int(get_num.search(file).group())\n",
    "\n",
    "# Get list of all processed JSON files from the traffic law directory\n",
    "file_list = os.listdir(r\"..\\data\\trafficLaw\")\n",
    "\n",
    "# Sort files numerically to maintain legal document order\n",
    "# This ensures articles are processed in their original sequence\n",
    "file_list.sort(key=files_num)\n",
    "\n",
    "# Load and combine all JSON files into a single list\n",
    "for file in file_list:\n",
    "    with open(os.path.join(r\"..\\data\\trafficLaw\", file), 'r', encoding='utf8') as f:\n",
    "        json_files.append(json.load(f))\n",
    "\n",
    "# Display the aggregated JSON data\n",
    "json_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. JSON File Aggregation\n",
    "\n",
    "### Collecting Processed Results\n",
    "After parallel processing, this section collects all individual JSON files and combines them into a single comprehensive dataset. The files are sorted numerically to maintain the original law structure order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a flattened list of all detected legal fields\n",
    "ultimate_json = []\n",
    "\n",
    "# Extract all detected fields from individual JSON files\n",
    "# This flattens the nested structure into a single list\n",
    "for json_file in json_files:\n",
    "    ultimate_json.extend(json_file['campos_detectados'])\n",
    "\n",
    "# Initialize sub_campos field for each element\n",
    "# This prepares the structure for hierarchical organization\n",
    "for element in ultimate_json:\n",
    "    element['sub_campos'] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Intermediate Results\n",
    "Save the flattened legal structure for backup and debugging purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the legal hierarchy in order of precedence\n",
    "# This list determines parent-child relationships in the legal structure\n",
    "hierarchy_list = [\"Título\", \"Capítulo\", \"Seção\", \"Artigo\", \"Parágrafo\", \"Inciso\", \"Item\"]\n",
    "\n",
    "def check_type_hierarchy(field1, field2):\n",
    "    \"\"\"\n",
    "    Compare two legal fields to determine their hierarchical relationship.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    field1, field2 : dict\n",
    "        Legal field objects containing 'tipo_do_campo' keys\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        'sub' if field2 should be a child of field1\n",
    "        'super' if field2 should be a parent of field1  \n",
    "        'equal' if fields are at the same hierarchy level\n",
    "    \"\"\"\n",
    "    print(field1, field2)\n",
    "    type1 = field1['tipo_do_campo']\n",
    "    type2 = field2['tipo_do_campo']\n",
    "    \n",
    "    # Compare positions in hierarchy list\n",
    "    if hierarchy_list.index(type1) < hierarchy_list.index(type2):\n",
    "        return 'sub'     # field2 is subordinate to field1\n",
    "    elif hierarchy_list.index(type1) > hierarchy_list.index(type2):\n",
    "        return 'super'   # field2 is superior to field1\n",
    "    else:\n",
    "        return 'equal'   # fields at same level\n",
    "\n",
    "def map_name_to_hierarchy(name):\n",
    "    \"\"\"\n",
    "    Map field type names to standardized hierarchy names using fuzzy matching.\n",
    "    \n",
    "    This function handles variations in AI-generated field type names by finding\n",
    "    the closest match in the standard hierarchy list using Levenshtein distance.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    name : str\n",
    "        Field type name to be normalized\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Standardized hierarchy name, or \"ERRO\" if no close match found\n",
    "    \"\"\"\n",
    "    min_dist = 100\n",
    "    closest = None\n",
    "    \n",
    "    # Find the closest match using string distance\n",
    "    for hierarchy in hierarchy_list:\n",
    "        dist = distance(name, hierarchy)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            closest = hierarchy\n",
    "    \n",
    "    # Reject matches that are too different (likely errors)\n",
    "    if min_dist >= 3:\n",
    "        return \"ERRO\"\n",
    "    return closest\n",
    "\n",
    "def get_json_element_by_address(json, address):\n",
    "    \"\"\"\n",
    "    Navigate to a specific element in nested JSON using address path.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    json : dict/list\n",
    "        The JSON structure to navigate\n",
    "    address : list\n",
    "        List of keys/indices representing the path to the target element\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict/list\n",
    "        The element at the specified address\n",
    "    \"\"\"\n",
    "    element = json\n",
    "    for i in address:\n",
    "        element = element[i]\n",
    "    return element\n",
    "\n",
    "def change_json_element_by_address(json, address, new_element):\n",
    "    \"\"\"\n",
    "    Update a specific element in nested JSON structure.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    json : dict/list\n",
    "        The JSON structure to modify\n",
    "    address : list\n",
    "        Path to the element to be changed\n",
    "    new_element : any\n",
    "        New value to assign to the element\n",
    "    \"\"\"\n",
    "    element = json\n",
    "    for i in address[:-1]:\n",
    "        element = element[i]\n",
    "    element[address[-1]] = new_element\n",
    "\n",
    "def create_hierarchy_json(field_list):\n",
    "    \"\"\"\n",
    "    Transform a flat list of legal fields into a hierarchical tree structure.\n",
    "    \n",
    "    This function implements the core logic for building the legal document hierarchy.\n",
    "    It processes fields sequentially and determines where each field should be placed\n",
    "    in the tree based on its type and relationship to previously processed fields.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Normalize field types using fuzzy matching\n",
    "    2. For each field, find the appropriate parent in the existing tree\n",
    "    3. Insert the field at the correct position maintaining hierarchy rules\n",
    "    4. Track insertion positions for efficient subsequent operations\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    field_list : list\n",
    "        Flat list of legal field dictionaries\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        Hierarchical structure with nested sub_campos relationships\n",
    "    \"\"\"\n",
    "    for i, field in enumerate(field_list):\n",
    "        # Normalize field type name using fuzzy matching\n",
    "        field['tipo_do_campo'] = map_name_to_hierarchy(field['tipo_do_campo'])\n",
    "        \n",
    "        # Skip fields that couldn't be properly classified\n",
    "        if field['tipo_do_campo'] == \"ERRO\":\n",
    "            continue\n",
    "        \n",
    "        # Initialize hierarchy with first valid field\n",
    "        if i == 0:\n",
    "            hierarchy_json = {'sub_campos': [field]}\n",
    "            last_added_address = ['sub_campos', 0]\n",
    "        else:\n",
    "            # Get the most recently added field for comparison\n",
    "            last_added = get_json_element_by_address(hierarchy_json, last_added_address)\n",
    "            \n",
    "            # Check if current field should be a child of the last added field\n",
    "            if check_type_hierarchy(last_added, field) == 'sub':\n",
    "                last_added['sub_campos'].append(field)\n",
    "                last_added_address = last_added_address + ['sub_campos', len(last_added['sub_campos']) - 1]\n",
    "            else:\n",
    "                # Find appropriate parent by traversing up the tree\n",
    "                added = False\n",
    "                for j in range(len(last_added_address)):\n",
    "                    # Create potential parent address by removing elements from current address\n",
    "                    new_last_added_address = last_added_address[:-j]\n",
    "                    \n",
    "                    # Skip invalid addresses\n",
    "                    if new_last_added_address and new_last_added_address[-1] == 'sub_campos':\n",
    "                        continue\n",
    "                    elif new_last_added_address == []:\n",
    "                        continue\n",
    "                    \n",
    "                    # Check if this parent can accommodate the new field\n",
    "                    new_last_added = get_json_element_by_address(hierarchy_json, new_last_added_address)\n",
    "                    print(new_last_added_address)\n",
    "                    \n",
    "                    if check_type_hierarchy(new_last_added, field) == 'sub':\n",
    "                        new_last_added['sub_campos'].append(field)\n",
    "                        last_added_address = new_last_added_address + ['sub_campos', len(new_last_added['sub_campos']) - 1]\n",
    "                        added = True\n",
    "                        break\n",
    "                \n",
    "                # If no suitable parent found, add to root level\n",
    "                if not added:\n",
    "                    hierarchy_json['sub_campos'].append(field)\n",
    "                    last_added_address = ['sub_campos', len(hierarchy_json['sub_campos']) - 1]\n",
    "    \n",
    "    return hierarchy_json['sub_campos']\n",
    "\n",
    "# Execute the hierarchical organization\n",
    "# This transforms the flat list into a proper legal document tree structure\n",
    "hierarchy_json = create_hierarchy_json(ultimate_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hierarchical Organization System\n",
    "\n",
    "### Legal Hierarchy Implementation\n",
    "This section implements a sophisticated system to organize legal elements into their proper hierarchical structure. The system:\n",
    "\n",
    "1. **Normalizes field types** using fuzzy string matching (Levenshtein distance)\n",
    "2. **Establishes parent-child relationships** based on legal precedence\n",
    "3. **Builds a tree structure** that mirrors the actual legal document organization\n",
    "\n",
    "### Hierarchy Levels (in order of precedence):\n",
    "1. **Título** (Title) - Highest level organizational unit\n",
    "2. **Capítulo** (Chapter) - Major subdivisions within titles  \n",
    "3. **Seção** (Section) - Subdivisions within chapters\n",
    "4. **Artigo** (Article) - Individual legal provisions\n",
    "5. **Parágrafo** (Paragraph) - Clarifications or extensions of articles\n",
    "6. **Inciso** (Subsection) - Enumerated items within articles/paragraphs\n",
    "7. **Item** (Item) - Detailed specifications within subsections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the complete hierarchical structure to JSON file\n",
    "# This represents the fully organized Brazilian Traffic Law structure\n",
    "with open(r\"cod_transito.json\", 'w', encoding='utf8') as f:\n",
    "    json.dump(hierarchy_json, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate vector database paths and values with references to later upload to a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate vector database dict\n",
    "vector_db_dict = {}\n",
    "\n",
    "def generate_vector_db_dict(hierarchy_json, vector_db_dict, address=[], texts_to_concat=[]):\n",
    "    for i, field in enumerate(hierarchy_json):\n",
    "        if len(field['sub_campos']) > 0:\n",
    "            generate_vector_db_dict(field['sub_campos'], vector_db_dict, address+[field['título_do_campo']], texts_to_concat+[field['texto']])\n",
    "        else:\n",
    "            vector_db_dict[\" - \".join(address+[field['título_do_campo']])] = \" \".join(texts_to_concat+[field['texto']])\n",
    "    \n",
    "    return vector_db_dict\n",
    "\n",
    "vector_db_dict = generate_vector_db_dict([hierarchy_json[16]], vector_db_dict)\n",
    "\n",
    "vector_db_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual data clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print only keys with \"Infração\", \"Penalidade\", \"Medida Administrativa\"\n",
    "vector_db_dict = {k: v for k, v in vector_db_dict.items() if not any(x in k.lower() for x in [\"penalidade\", \"medida administrativa\"])}\n",
    "vector_db_dict = {k: v for k, v in vector_db_dict.items() if not any(x in v.lower() for x in ['vetado', 'revogado', 'suspenso'])}\n",
    "\n",
    "# replace \" - Infração\" in keys with \"\"\n",
    "vector_db_dict = {k.replace(\" - Infração\", \"\"): v for k, v in vector_db_dict.items()}\n",
    "vector_db_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save vector_db dict as json file\n",
    "with open(r\"vector_db_dict.json\", 'w', encoding='utf8') as f:\n",
    "    json.dump(vector_db_dict, f, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
